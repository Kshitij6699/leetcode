{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|student_name|\n",
      "+----------+------------+\n",
      "|         1|       Alice|\n",
      "|         2|         Bob|\n",
      "|        13|        John|\n",
      "|         6|        Alex|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "(1,'Alice'),\n",
    "(2,'Bob'),\n",
    "(13,'John'),\n",
    "(6,'Alex')\n",
    "]\n",
    "col=['student_id','student_name']\n",
    "\n",
    "students=spark.createDataFrame(data,col)\n",
    "students.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Subject_name|\n",
      "+------------+\n",
      "|        Math|\n",
      "|     Physics|\n",
      "| Programming|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[('Math',),('Physics',),('Programming',)]\n",
    "col=['Subject_name']\n",
    "\n",
    "subjects=spark.createDataFrame(data,col)\n",
    "subjects.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|student_id|subject_name|\n",
      "+----------+------------+\n",
      "|         1|        Math|\n",
      "|         1|     Physics|\n",
      "|         1| Programming|\n",
      "|         2| Programming|\n",
      "|         1|     Physics|\n",
      "|         1|        Math|\n",
      "|        13|        Math|\n",
      "|        13| Programming|\n",
      "|        13|     Physics|\n",
      "|         2|        Math|\n",
      "|         1|        Math|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[\n",
    "(1,\"Math\"),\n",
    "(1,'Physics'),\n",
    "(1,'Programming'),\n",
    "(2,'Programming'),\n",
    "(1,'Physics'),\n",
    "(1,'Math'),\n",
    "(13,'Math'),\n",
    "(13,'Programming'),\n",
    "(13,'Physics'),\n",
    "(2,'Math'),\n",
    "(1,'Math')\n",
    "]\n",
    "\n",
    "columns = ['student_id','subject_name']\n",
    "\n",
    "examinations = spark.createDataFrame(data,columns)\n",
    "examinations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+\n",
      "|student_id|student_name|Subject_name|\n",
      "+----------+------------+------------+\n",
      "|         1|       Alice|     Physics|\n",
      "|         1|       Alice| Programming|\n",
      "|         1|       Alice|        Math|\n",
      "|         2|         Bob| Programming|\n",
      "|         2|         Bob|        Math|\n",
      "|         2|         Bob|     Physics|\n",
      "|         6|        Alex| Programming|\n",
      "|         6|        Alex|     Physics|\n",
      "|         6|        Alex|        Math|\n",
      "|        13|        John| Programming|\n",
      "|        13|        John|        Math|\n",
      "|        13|        John|     Physics|\n",
      "+----------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDataFrame = (students\n",
    "                .crossJoin(subjects)\n",
    "                # .join(examinations, 'student_id','left')\n",
    "                .orderBy('student_id'))\n",
    "joinDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+----------+------------+\n",
      "|student_id|student_name|Subject_name|student_id|subject_name|\n",
      "+----------+------------+------------+----------+------------+\n",
      "|         1|       Alice|        Math|         1|        Math|\n",
      "|         1|       Alice|        Math|         1|        Math|\n",
      "|         1|       Alice|        Math|         1|        Math|\n",
      "|         1|       Alice|     Physics|         1|     Physics|\n",
      "|         1|       Alice|     Physics|         1|     Physics|\n",
      "|         1|       Alice| Programming|         1| Programming|\n",
      "|         2|         Bob|        Math|         2|        Math|\n",
      "|         2|         Bob|     Physics|      NULL|        NULL|\n",
      "|         2|         Bob| Programming|         2| Programming|\n",
      "|         6|        Alex|        Math|      NULL|        NULL|\n",
      "|         6|        Alex|     Physics|      NULL|        NULL|\n",
      "|         6|        Alex| Programming|      NULL|        NULL|\n",
      "|        13|        John|        Math|        13|        Math|\n",
      "|        13|        John|     Physics|        13|     Physics|\n",
      "|        13|        John| Programming|        13| Programming|\n",
      "+----------+------------+------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDataFrame = (students\n",
    "                .crossJoin(subjects)\n",
    "                .join(examinations, ((students['student_id'] == examinations['student_id']) & (examinations['subject_name'] == subjects['subject_name'])),'left')\n",
    "                .groupBy(students['student_id'], examinations['subject_name']).count().withColumnRename('count','attended_exams')\n",
    "                .select(students['student_id'],students['student_name'],examinations['subject_name'],'attended_exams')\n",
    "                .orderBy(students['student_id']))\n",
    "joinDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinDataFrame = (students\n",
    "                .crossJoin(subjects)\n",
    "                .join(examinations, ((students['student_id'] == examinations['student_id']) & (examinations['subject_name'] == subjects['subject_name'])),'left')\n",
    "                .groupBy(students['student_id'], students['student_name'],subjects['subject_name']).agg(F.coalesce(F.count(examinations['student_id']),F.lit(0).alias('attended_exam'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column student_name#1 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m select_df \u001b[38;5;241m=\u001b[39m \u001b[43mjoinDataFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoinDataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstudent_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstudents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstudent_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoinDataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoinDataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudent_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudents\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudent_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoinDataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoinDataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcoalesce(count(student_id), 0 AS attended_exam)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[0;32m      4\u001b[0m                       \u001b[38;5;241m.\u001b[39morderBy(joinDataFrame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Show the result\u001b[39;00m\n\u001b[0;32m      7\u001b[0m select_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   3184\u001b[0m \n\u001b[0;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column student_name#1 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
     ]
    }
   ],
   "source": [
    "select_df = joinDataFrame.join(students, joinDataFrame[\"student_id\"] == students[\"student_id\"], how='left')\\\n",
    "                      .join(subjects, joinDataFrame[\"subject_name\"] == subjects[\"subject_name\"], how='left')\\\n",
    "                      .select(joinDataFrame['student_id'], students['student_name'], joinDataFrame['subject_name'], joinDataFrame['coalesce(count(student_id), 0 AS attended_exam)'])\\\n",
    "                      .orderBy(joinDataFrame['student_id'])\n",
    "\n",
    "# Show the result\n",
    "select_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
